---
title: "Process Data"
author: "Sam Matthews"
format:
  html:
    theme: 
      light: cosmo      # pick any light Bootswatch theme
      dark: solar      # pick any dark Bootswatch theme
    toc: true
    code-fold: show
    self-contained: true
execute:
  echo: true
  warning: false
  message: false
editor: visual
---

## Set Up

```{r}
# Packages ---------------------------------------------------------------
# (uncomment next 2 lines if you want auto-install)
# pkgs <- c("leaflet","terra","sf","leafem","raster")
# to_install <- pkgs[!sapply(pkgs, requireNamespace, quietly = TRUE)]; if (length(to_install)) install.packages(to_install)

library(terra)    # raster
library(sf)       # vectors
library(leaflet)  # web map
library(leafem)   # helpers for raster in leaflet
library(raster)   # addRasterImage prefers RasterLayer
library(igraph)
library(units)
library(ggplot2)
library(smoothr)
library(dplyr)
library(gdistance)
library(readxl)
```

## Read Data

```{r}
coralSDM.Ahya <- terra::rast("data/maxent_predrast_GBR_AhyaD_015_lq2.tif")
coralSDM.Aspat <- terra::rast("data/maxent_predrast_GBR_Aspat_015_lq2.tif")
coralSDM.Aten <- terra::rast("data/maxent_predrast_GBR_Aten_015_lq2.tif")
crs(coralSDM.Ahya)
#John Brewer
bb_wgs <- ext(147.0, 147.1, -18.66, -18.6)
# Britomart
# bb_wgs <- ext(145.65, 145.88, -16.48, -16.33) 
# 2) Turn extent -> polygon with CRS, then project to raster CRS (MGA55)
bb_wgs_poly <- as.polygons(bb_wgs, crs = "EPSG:4326")
bb_mga_poly <- project(bb_wgs_poly, crs(coralSDM.Ahya))


coralSDM.Ahya_crop <- crop(coralSDM.Ahya, bb_mga_poly)
coralSDM.Aspat_crop <- crop(coralSDM.Aspat, bb_mga_poly)
coralSDM.Aten_crop <- crop(coralSDM.Aten, bb_mga_poly)

plot(coralSDM.Ahya_crop)
plot(coralSDM.Aspat_crop)
plot(coralSDM.Aten_crop)
```

```{r Read in Raster}
coralSDM.Ahya  <- rast("data/maxent_predrast_GBR_AhyaD_015_lq2.tif")
coralSDM.Aspat <- rast("data/maxent_predrast_GBR_Aspat_015_lq2.tif")
coralSDM.Aten  <- rast("data/maxent_predrast_GBR_Aten_015_lq2.tif")

crs(coralSDM.Ahya)
crs(coralSDM.Aspat)
crs(coralSDM.Aten)


# list the 30 m rasters
env_files <- list.files(
  "data/GBR_bathymetryDEM_ACA_UTM55_030_crop",
  pattern = "\\.tif$",
  full.names = TRUE
)

env30 <- rast(env_files)
env30

# Check CRS
crs(env30)

#Geomorphic and Benthic Data
geomorph_rast <- rast("data/GBR10_GBRMP_Geomorphic/GBR10 GBRMP Geomorphic.tif")
benthic_rast <- rast("data/GBR10_GBRMP_Benthic/GBR10 GBRMP Benthic.tif")
crs(geomorph_rast); crs(benthic_rast)
# project/align to predictor grid
benthic_rast <- project(benthic_rast, predictors_reef[[1]], method = "near")
geomorph_rast <- project(geomorph_rast, predictors_reef[[1]], method = "near")
benthic_rast <- resample(benthic_rast, predictors_reef[[1]], method = "near")
geomorph_rast <- resample(geomorph_rast, predictors_reef[[1]], method = "near")
names(benthic_rast) <- "benthic_class"

# add to predictor stack
predictors_reef <- c(predictors_reef, benthic_rast)

```

### Need to add in ReefGuide data



## Create Template Grid


```{r Create Template Grid}
template30 <- env30[[1]]
res(template30)   # should be ~30 m
ext(template30)
crs(template30)   # should now be EPSG:7855

# Align resolution/extent to the 30 m template
Ahya_30  <- resample(coralSDM.Ahya,  template30, method = "bilinear")
Aspat_30 <- resample(coralSDM.Aspat, template30, method = "bilinear")
Aten_30  <- resample(coralSDM.Aten,  template30, method = "bilinear")


all.equal(crs(Ahya_30),  crs(template30))
all.equal(crs(Aspat_30), crs(template30))
all.equal(crs(Aten_30),  crs(template30))
res(Ahya_30); res(template30)

# keep env layers separate if you like
names(env30)  # rename if needed to sensible names

coral_sdm_30 <- c(Ahya_30, Aspat_30, Aten_30)
names(coral_sdm_30) <- c("SDM_Ahya", "SDM_Aspat", "SDM_Aten")

predictors_terra <- c(env30, coral_sdm_30)
predictors_terra

library(raster)

predictors_raster <- raster::stack(c(env30, coral_sdm_30))
predictors_raster


```

##Mask Rasters to GBR Features

```{r}
############################################################
# Mask full predictor stack to reef polygons
# - GeoPackage: "data/Great_Barrier_Reef_Features.gpkg"
# - Reef polygons are in EPSG:7844 (GDA2020 geographic)
# - Rasters are also in EPSG:7844 (NOT 7855 as first told)
############################################################

library(terra)
library(sf)
library(dplyr)
library(tidyterra)

## 1. Read reef polygons -----------------------------------

# Option 1: if there's only one layer in the GeoPackage
reef_poly <- terra::vect("data/Great_Barrier_Reef_Features.gpkg") |>
  filter(FEAT_NAME=="Reef")
attr_reefs <- as.data.frame(reef_poly)

# Option 2: if there are multiple layers and you know the layer name:
# reef_poly <- terra::vect("data/Great_Barrier_Reef_Features.gpkg",
#                          layer = "Great_Barrier_Reef_Features")

# Inspect CRS of polygons
crs(reef_poly)



# Should show something like: "EPSG:7844"

## 2. Check CRS of rasters ---------------------------------

# If the CRSs differ, reproject reef_poly to the raster CRS:
if (crs(reef_poly) != crs(predictors_terra)) {
  message("Reprojecting reef polygons to raster CRS...")
  reef_poly <- terra::project(reef_poly, crs(predictors_terra))
}

cat("\nRaster extent:\n")
print(ext(predictors_terra))
cat("\nReef extent:\n")
print(ext(reef_poly))

cat("\nIs raster lon/lat? ", is.lonlat(predictors_terra), "\n")
cat("Is reef lon/lat?   ", is.lonlat(reef_poly), "\n")

## 3. Crop + mask the whole SpatRaster ---------------------
plot(predictors_terra[[12]], main = "Predictor unmasked")
# # First crop to the reef polygon extent – faster than masking full GBR
predictors_reef_crop <- terra::crop(predictors_terra, reef_poly)
plot(predictors_terra[[12]], main = "Predictor cropped")
# Then apply mask: cells OUTSIDE reef polygons become NA
predictors_reef <- terra::mask(predictors_terra, reef_poly)

# Quick visual check of one layer
plot(predictors_reef[[1]], main = "Predictor masked to reef areas")
plot(predictors_reef[[12]], main = "Predictor masked to reef areas")


# Check it’s sane once before saving
predictors_reef
names(predictors_reef)

# Save to disk as a multi-band GeoTIFF
writeRaster(
  predictors_reef,
  filename = "data/predictors_reef.tif",
  overwrite = TRUE,
  gdal = c("COMPRESS=LZW")  # optional compression
)
```



```{r Load survey data}
dat.cull <- read_excel("data/250929_COTS-Manta-Cull-RHIS-Data-Matthews-and-Schlawinsky.xlsx", sheet = 4) |>
  mutate(VoyageTitle = as.factor(VoyageTitle),
         YearQr = zoo::as.yearqtr(SurveyDate),
         Year = lubridate::year(SurveyDate),
         Total = Cohort1 +Cohort2+Cohort3+Cohort4,
         CPUE = Total/Bottomtime) %>%
  group_by(ReefName, CullSiteName, Year, Latitude, Longitude) %>%
  summarise(MaxCPUE = max(CPUE),
            CPUE = sum(Total, na.rm = T)/sum(Bottomtime, na.rm = T),
            Total = sum(Total), 
            Bottomtime = sum(Bottomtime)) %>%
  filter(!is.na(Longitude))

dat.cull_sf <- dat.cull |> 
  st_as_sf(
    coords = c("Longitude", "Latitude"),  # <- CHANGE if your cols are named differently
    crs    = 4326              # WGS84 geographic
  )

plot(dat.cull_sf)

# Your predictor rasters are in EPSG:7844 (GDA2020 geographic),
# so we transform the survey points to that CRS:
survey_sf <- dat.cull_sf |> st_transform(7844)

```




```{r}
############################################################
# COTS SDM with XGBoost
# - Model 1: "Problematic" COTS presence (classification)
# - Model 2: COTS density per ha (regression)
# - Includes raster predictors + reef/sector outbreak covariates
# - Uses spatially blocked cross-validation (blockCV)
############################################################

## 0. Packages ------------------------------------------------------------

library(terra)        # raster predictors
library(sf)           # spatial survey data
library(dplyr)        # data wrangling
library(tidyr)
library(tidymodels)   # modelling framework
library(blockCV)      # spatial CV
library(ggplot2)      # plotting (optional)
library(xgboost)      # xgboost engine for parsnip

tidymodels::tidymodels_prefer()

## 1. Inputs & assumptions -----------------------------------------------
# You need to have these objects already created / loaded:

# 1) predictors_terra: SpatRaster with all environmental predictors
#    (bathymetry, rugosity, SDMs, etc.) in EPSG:7855, aligned to your grid.
#    e.g. from earlier:
#    predictors_terra <- c(env30, coral_sdm_30)

# 2) survey_sf: sf object with one row per survey / reef-visit / tow:
#      - cots_density_ha : numeric COTS density per ha
#      - reef_id         : reef identifier (factor/character)
#      - sector          : sector ID / region (factor/character)
#      - year            : survey year (numeric or factor)
#      - geometry        : POINT (or centroid) in EPSG:7855

# 3) outbreak_pred_df (optional): data frame with reef/year-level metrics:
#      - reef_id, year
#      - e.g. cots_outbreak_prob, sector_mean_density, etc.
#
# Make sure CRS(predictors_terra) and st_crs(survey_sf) are both EPSG:7855.




## 2. Define response variables ------------------------------------------

# 2.1 Threshold for "problematic" COTS densities
#     (set according to program definitions)
problem_threshold <- 0.02   # example: 30 COTS/ha; adjust to your threshold

# Ensure density is numeric and create binary response
survey_sf <- survey_sf %>%
  mutate(
    cots_density_cpue = as.numeric(CPUE),
    # Binary target: 1 = problematic density, 0 = below threshold
    cots_problem = if_else(cots_density_cpue >= problem_threshold, 1L, 0L)
  )


## 3. Extract raster predictors at survey locations ----------------------

# 3.1 Ensure CRSs match (project survey_sf to raster CRS if needed)
survey_sf <- st_transform(survey_sf, crs(predictors_reef))


# 3.2 Add explicit ID for safe joining
survey_sf$ID <- seq_len(nrow(survey_sf))

# 3.3 Extract raster values
pred_vals <- terra::extract(predictors_reef, vect(survey_sf))

#####IM HERE#####


# 'pred_vals' has column 'ID' from the vect() input
# Join raster predictors back to survey attributes (without geometry)
survey_df <- survey_sf %>%
  st_drop_geometry() %>%
  left_join(pred_vals, by = "ID")

# 3.4 Add outbreak / sector-level predictors (if available)
#     outbreak_pred_df: one row per reef_id & year with extra covariates
# THis owens outbreaks model to be included later
if (exists("outbreak_pred_df")) {
  survey_df <- survey_df %>%
    left_join(outbreak_pred_df, by = c("reef_id", "year"))
}

# 3.5 Drop ID (no longer needed)
survey_df <- survey_df %>% dplyr::select(-ID)

# Quick check
glimpse(survey_df)


## 4. Spatial blocking for cross-validation -------------------------------

# 4.1 Use the sf object (with geometry) for blockCV
#     We only need the cots_problem response + geometry here.
survey_for_blocks <- survey_sf %>%
  dplyr::select(cots_problem)

# 4.2 Define spatial blocks
#     - theRange: spatial block size in metres; adjust sensibly (e.g. 50–100 km)
set.seed(123)
sb <- spatialBlock(
  speciesData = survey_for_blocks,
  species     = "cots_problem",
  theRange    = 50000,    # ~50 km blocks (example)
  k           = 5,        # 5-fold CV
  selection   = "random",
  showBlocks  = FALSE
)

# blockCV returns foldID vector (one per observation)
survey_df$fold_id <- sb$foldID

# You can reuse these fold IDs for both classification & regression models.


## 5. Prepare modelling data frames --------------------------------------

# 5.1 Identify predictor column names from raster stack
pred_cols <- names(predictors_reef)

# 5.2 Identify outbreak covariates (if present)
outbreak_cols <- character(0)
if (exists("outbreak_pred_df")) {
  outbreak_cols <- setdiff(names(outbreak_pred_df), c("reef_id", "year"))
}

# 5.3 Build base modelling data frame
model_df <- survey_df %>%
  dplyr::select(
    cots_problem,          # classification target
    cots_density_cpue,       # regression target
    ReefName,
    # sector,
    Year,
    fold_id,
    dplyr::all_of(pred_cols),
    dplyr::all_of(outbreak_cols)
  )

# 5.4 Remove rows with missing response or predictors
model_df <- model_df %>%
  filter(!is.na(cots_problem), !is.na(cots_density_cpue)) %>%
  drop_na(dplyr::all_of(pred_cols))

# Optional: drop rows with rare / unused factor levels
model_df <- model_df %>%
  mutate(
    cots_problem = as.factor(cots_problem),
    ReefName = as.factor(ReefName),
    # sector  = as.factor(sector),
    # Year    = as.factor(Year)
  ) %>% ungroup() %>%
  #need to check how to add factors
  dplyr::select(-ReefName, - CullSiteName)

glimpse(model_df)


## 6. Model 1: XGBoost classification (problematic COTS) -----------------

# 6.1 CV folds based on spatial blocks
set.seed(123)
cv_folds_cls <- group_vfold_cv(model_df, group = fold_id, v = 5)

# 6.2 Recipe: preprocess predictors
rec_cls <- recipe(cots_problem ~ ., data = model_df %>% dplyr::select(-cots_density_cpue)) %>%
  # fold_id is for CV grouping only; remove as predictor
  update_role(fold_id, new_role = "id") %>%
  step_rm(fold_id) %>%
  # near-zero variance predictors
  step_zv(all_predictors()) %>%
  # normalise numeric predictors (optional but often helpful)
  step_normalize(all_numeric_predictors())

# 6.3 XGBoost model specification (tuned)
xgb_spec_cls <- boost_tree(
  trees          = 1500,
  tree_depth     = tune(),
  learn_rate     = tune(),
  loss_reduction = tune(),
  min_n          = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# 6.4 Workflow
wf_cls <- workflow() %>%
  add_model(xgb_spec_cls) %>%
  add_recipe(rec_cls)

# 6.5 Hyperparameter tuning with blocked CV
set.seed(123)
grid_cls <- grid_space_filling(
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  min_n(),
  size = 30
)

res_cls <- tune_grid(
  wf_cls,
  resamples = cv_folds_cls,
  grid      = grid_cls,
  metrics   = metric_set(roc_auc, pr_auc, accuracy)
)

# 6.6 Select best hyperparameters (maximising ROC AUC)
best_cls <- select_best(res_cls, metric = "roc_auc")

# 6.7 Finalise workflow and fit on all data
final_wf_cls <- finalize_workflow(wf_cls, best_cls)

set.seed(123)
fit_cls <- final_wf_cls %>%
  fit(data = model_df)

fit_cls

# 6.8 Optional: model performance using CV results
model_metrics <- collect_metrics(res_cls)

# 6.9 Optional: variable importance plot
vip_cls <- fit_cls %>%
  extract_fit_parsnip() %>%
  vip::vip(num_features = 20)

print(vip_cls)

```

### Predict and Plot

```{r}
############################################################
# Predict XGBoost classification to reef raster grid
# and plot in leaflet
############################################################

library(tidymodels)

# e.g. predict for 2025, or for the mean/most recent year in the data
target_year <- 2025
target_year <- max(model_df$Year, na.rm = TRUE)
# sanity check using your training data
range(model_df$Year, na.rm = TRUE)
library(terra)
library(tidymodels)

# predictors_reef: SpatRaster with all numeric predictors used in the model
#                  (no year layer, just env vars, outbreak metrics, etc.)
# fit_cls: fitted tidymodels workflow (classification model)

# 1) Define prediction function for terra::predict
# prediction function: x = matrix/data.frame of raster values
pred_fun <- function(model, v) {
  # model: your fitted tidymodels workflow (fit_cls)
  # v    : matrix/data.frame of raster predictors for a chunk
  #        rows = cells, cols = layers in predictors_reef

  # Coerce to data.frame
  v <- as.data.frame(v)

  # Give columns the raster layer names (must match what the recipe expects)
  names(v) <- names(predictors_reef)

  # Add numeric Year column expected by the model
  v$Year <- target_year
  
   # Add dummy fold_id so hardhat is happy
  # Copy type from training data:
  v$fold_id <- model_df$fold_id[1]

  # Use the tidymodels workflow's predict()
  p <- predict(
    model,
    new_data = v,
    type     = "prob"
  )

  # Return numeric vector of probabilities for the "1" class
  as.numeric(p$.pred_1)
}

# # Take a small sample of raster values
# test_mat <- terra::values(predictors_reef, mat = TRUE)[1:10, , drop = FALSE]
# 
# # Run pred_fun on this sample
# test_pred <- pred_fun(test_mat)
# test_pred

names(model_df)         # make sure it's "Year" not "year"
str(model_df$Year)      # should be numeric (or integer)
str(model_df$fold_id)
# likely integer or factor
# 2) Run chunked prediction over the raster
prob_rast <- terra::predict(
  predictors_reef,  # SpatRaster
  fit_cls,          # model argument
  fun   = pred_fun,
  na.rm = TRUE,
  cores = 1         # keep 1 to avoid xgboost pointer weirdness
)

names(prob_rast) <- "prob_problem"
prob_rast
plot(prob_rast)

# Optional: check CRS (ArcGIS can handle 7844 or 4326 just fine)
crs(prob_rast)

# 1) Write out as GeoTIFF in the raster's native CRS
writeRaster(
  prob_rast,
  filename = "outputs/COTS_prob_problem.tif",
  overwrite = TRUE,
  gdal = c("COMPRESS=LZW")   # optional but makes file smaller
)

```

```{r Plot confusion nd metrics}
# -----------------------------------------------------------
# 1. Get predictions from fitted classification model
# -----------------------------------------------------------

# --------------------------------------------------
# 1. Choose your threshold
# --------------------------------------------------
thr <- 0.45   # change this (e.g. 0.5, 0.3, etc.)

# --------------------------------------------------
# 2. Get probabilities and build hard class with thr
# --------------------------------------------------

# Probabilities from the fitted workflow
pred_prob <- predict(
  fit_cls,
  new_data = model_df,
  type     = "prob"
)

# Combine with original data and build custom class
aug_cls <- model_df %>%
  bind_cols(pred_prob) %>%
  mutate(
    cots_problem = factor(cots_problem, levels = c(0, 1)),
    .pred_class  = factor(
      if_else(.pred_1 >= thr, "1", "0"),
      levels = c("0", "1")
    ),
    truth_num = as.numeric(as.character(cots_problem))
  )

# -----------------------------------------------------------
# 2. Overall metrics
# -----------------------------------------------------------

# ROC AUC and PR AUC use probabilities
roc_auc_val <- roc_auc(
  aug_cls,
  truth   = cots_problem,
  .pred_1               # probability of class "1" (problematic)
)$.estimate

pr_auc_val <- pr_auc(
  aug_cls,
  truth   = cots_problem,
  .pred_1
)$.estimate

# Threshold-based metrics (@ 0.5, since .pred_class already reflects that)
acc_val <- accuracy(
  aug_cls,
  truth   = cots_problem,
  estimate = .pred_class
)$.estimate

prec_val <- precision(
  aug_cls,
  truth   = cots_problem,
  estimate = .pred_class,
  event_level = "second"   # "1" is the second level
)$.estimate

recall_val <- recall(
  aug_cls,
  truth   = cots_problem,
  estimate = .pred_class,
  event_level = "second"
)$.estimate

f1_val <- f_meas(
  aug_cls,
  truth   = cots_problem,
  estimate = .pred_class,
  event_level = "second"
)$.estimate

# RMSE and R² on probs vs 0/1 truth
rmse_val <- rmse(
  aug_cls,
  truth   = truth_num,
  estimate = .pred_1
)$.estimate

rsq_val <- rsq_trad(
  aug_cls,
  truth   = truth_num,
  estimate = .pred_1
)$.estimate

metrics_summary <- tibble(
  metric = c("ROC AUC", "PR AUC", "Accuracy", "Precision", "Recall", "F1",
             "RMSE (prob vs 0/1)", "R² (prob vs 0/1)"),
  value  = c(roc_auc_val, pr_auc_val, acc_val, prec_val, recall_val,
             f1_val, rmse_val, rsq_val)
)

print(metrics_summary)

# -----------------------------------------------------------
# 3. Confusion matrix heatmap with RMSE & R² annotated
# -----------------------------------------------------------

# -----------------------------------------------------------
# Confusion matrix as a tibble
# -----------------------------------------------------------
cm <- conf_mat(
  aug_cls,
  truth    = cots_problem,
  estimate = .pred_class
)

# Extract the underlying matrix and turn into a long data frame
cm_mat <- cm$table

cm_df <- as.data.frame(cm_mat) #%>%


# Overall percentages (each cell as % of all samples)
cm_df <- cm_df %>%
  mutate(
    pct = Freq / sum(Freq),
    label = sprintf("%.1f%%\n(n=%d)", pct * 100, Freq)
  )

# -----------------------------------------------------------
# Plot: predictions on x-axis (0 -> 1), truth on y-axis (0 -> 1)
# -----------------------------------------------------------
cm_plot <- ggplot(cm_df, aes(x = Prediction, y = Truth, fill = pct)) +
  geom_tile(color = "grey70") +
  geom_text(aes(label = label), size = 3.5) +
  scale_fill_gradient(
    name = "% of samples",
    low  = "white",
    high = "steelblue",
    limits = c(0, max(cm_df$pct))
  ) +
  scale_x_discrete(position = "bottom") +
  labs(
    x = "Predicted class",
    y = "True class",
    title    = "COTS 'problematic' classification – confusion matrix",
    subtitle = "Cell values are overall percentages and counts"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11),
    plot.title = element_text(face = "bold")
  ) +
  annotate(
    "text",
    x      = 0.55,
    y      = 2.45,
    hjust  = 0,
    vjust  = 1,
    label  = sprintf(
      "ROC AUC: %.3f | PR AUC: %.3f\nRMSE: %.3f | R²: %.3f | ACC: %.3f",
      roc_auc_val, pr_auc_val, rmse_val, rsq_val, acc_val
    ),
    size   = 3.3
  )

print(cm_plot)


aug_cls <- model_df %>%
  bind_cols(pred_prob) %>%
  mutate(cots_problem = factor(cots_problem, levels = c(0, 1)))

ggplot(aug_cls, aes(x = .pred_1, fill = cots_problem)) +
  geom_histogram(position = "identity", alpha = 0.4, bins = 30) +
  labs(x = "P(problematic)", fill = "Truth") +
  theme_minimal()


# Helper: compute metrics at a given threshold ----------------------------
metrics_at_threshold <- function(data, thr) {
  data_thr <- data %>%
    mutate(
      .pred_class_thr = factor(
        if_else(.pred_1 >= thr, "1", "0"),
        levels = c("0", "1")
      )
    )

  tibble(
    threshold = thr,
    accuracy  = accuracy(data_thr, cots_problem, .pred_class_thr)$.estimate,
    precision = precision(data_thr, cots_problem, .pred_class_thr,
                          event_level = "second")$.estimate,
    recall    = recall(data_thr, cots_problem, .pred_class_thr,
                       event_level = "second")$.estimate,
    f1        = f_meas(data_thr, cots_problem, .pred_class_thr,
                       event_level = "second")$.estimate
  )
}

# Example: compare 0.5 vs 0.4 --------------------------------------------
thr_results <- bind_rows(
  metrics_at_threshold(aug_cls, 0.5),
  metrics_at_threshold(aug_cls, 0.4)
)

thr_results

thresholds <- seq(0.1, 0.9, by = 0.05)

thr_grid <- purrr::map_dfr(thresholds, ~metrics_at_threshold(aug_cls, .x))

ggplot(thr_grid, aes(threshold)) +
  geom_line(aes(y = precision, colour = "precision")) +
  geom_line(aes(y = recall,    colour = "recall")) +
  geom_line(aes(y = f1,        colour = "F1")) +
  scale_colour_manual(values = c("precision" = "red",
                                 "recall"    = "blue",
                                 "F1"        = "black")) +
  labs(y = "Metric value", colour = "Metric",
       title = "Precision/Recall/F1 vs threshold")

```


```{r Debug Predict}
class(predictors_reef)
predictors_reef
names(predictors_reef)
nlyr(predictors_reef)
ext(predictors_reef)
crs(predictors_reef)
# Take a small block of raster values
vals <- terra::values(predictors_reef, mat = TRUE)   # matrix: rows = cells, cols = layers
dim(vals)
# e.g. 5M rows x 15 cols

# Look at a small sample
test_mat <- vals[1:10, , drop = FALSE]
str(test_mat)

# Convert that to a data.frame with layer names
test_df <- as.data.frame(test_mat)
names(test_df) <- names(predictors_reef)
str(test_df)

# Add Year as numeric like your model expects
test_df$Year <- target_year  # make sure the name & type match your model

# Now see if the workflow predict works on this
p_test <- predict(
  fit_cls,
  new_data = test_df,
  type     = "prob"
)
head(p_test)

```






```{r Model 2 COTS Density}
## 7. Model 2: XGBoost regression (COTS density) -------------------------

# 7.1 Optionally transform density (e.g. log to stabilise variance)
model_df <- model_df %>%
  mutate(
    cots_density_log = log1p(cots_density_cpue)  # log(1 + density)
  )

hist(model_df$cots_density_log)

# 7.2 CV folds: reuse same spatial folds for comparability
cv_folds_reg <- group_vfold_cv(model_df, group = fold_id, v = 5)

# 7.3 Recipe for regression
rec_reg <- recipe(cots_density_log ~ ., data = model_df %>% dplyr::select(-cots_density_cpue, -cots_problem)) %>%
  update_role(fold_id, new_role = "id") %>%
  step_rm(fold_id) %>%
  # step_rm(cots_density_cpue, cots_problem) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# 7.4 XGBoost spec (regression)
xgb_spec_reg <- boost_tree(
  trees          = 1500,
  tree_depth     = tune(),
  learn_rate     = tune(),
  loss_reduction = tune(),
  min_n          = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# 7.5 Workflow
wf_reg <- workflow() %>%
  add_model(xgb_spec_reg) %>%
  add_recipe(rec_reg)

# 7.6 Tuning grid (can re-use structure)
set.seed(123)
grid_reg <- grid_space_filling(
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  min_n(),
  size = 10
)

res_reg <- tune_grid(
  wf_reg,
  resamples = cv_folds_reg,
  grid      = grid_reg,
  metrics   = metric_set(rmse, rsq)
)

# 7.7 Select best hyperparameters (minimising RMSE)
best_reg <- select_best(res_reg, metric = "rmse")

# 7.8 Finalise and fit
final_wf_reg <- finalize_workflow(wf_reg, best_reg)

set.seed(123)
fit_reg <- final_wf_reg %>%
  fit(data = model_df)

fit_reg

# 7.9 CV metrics for regression
collect_metrics(res_reg)

```


```{r}
############################################################
# Predict XGBoost classification to reef raster grid
# and plot in leaflet
############################################################

library(tidymodels)

# e.g. predict for 2025, or for the mean/most recent year in the data
target_year <- 2025
target_year <- max(model_df$Year, na.rm = TRUE)
# sanity check using your training data
range(model_df$Year, na.rm = TRUE)
library(terra)
library(tidymodels)

# predictors_reef: SpatRaster with all numeric predictors used in the model
#                  (no year layer, just env vars, outbreak metrics, etc.)
# fit_cls: fitted tidymodels workflow (classification model)

# 1) Define prediction function for terra::predict
# prediction function: x = matrix/data.frame of raster values
pred_fun.dens <- function(model, v) {
  # model: your fitted tidymodels workflow (fit_cls)
  # v    : matrix/data.frame of raster predictors for a chunk
  #        rows = cells, cols = layers in predictors_reef

  # Coerce to data.frame
  v <- as.data.frame(v)

  # Give columns the raster layer names (must match what the recipe expects)
  names(v) <- names(predictors_reef)

  # Add numeric Year column expected by the model
  v$Year <- target_year
  
   # Add dummy fold_id so hardhat is happy
  # Copy type from training data:
  v$fold_id <- model_df$fold_id[1]

  # Use the tidymodels workflow's predict()
  p <- predict(
    model,
    new_data = v,
    type     = "numeric"
  )

  # Return numeric vector of probabilities for the "1" class
  as.numeric(p$.pred)
}

# # Take a small sample of raster values
# test_mat <- terra::values(predictors_reef, mat = TRUE)[1:10, , drop = FALSE]
# 
# # Run pred_fun on this sample
# test_pred <- pred_fun(test_mat)
# test_pred

names(model_df)         # make sure it's "Year" not "year"
str(model_df$Year)      # should be numeric (or integer)
str(model_df$fold_id)

# likely integer or factor
# 2) Run chunked prediction over the raster
prob_rast.dens <- terra::predict(
  predictors_reef,  # SpatRaster
  fit_reg,          # model argument
  fun   = pred_fun.dens,
  na.rm = TRUE,
  cores = 1         # keep 1 to avoid xgboost pointer weirdness
)

names(prob_rast.dens) <- "prob_problem"
prob_rast.dens
plot(prob_rast.dens)

# Optional: check CRS (ArcGIS can handle 7844 or 4326 just fine)
crs(prob_rast)

# 1) Write out as GeoTIFF in the raster's native CRS
# writeRaster(
#   prob_rast.dens,
#   filename = "outputs/COTS_prob_problem.tif",
#   overwrite = TRUE,
#   gdal = c("COMPRESS=LZW")   # optional but makes file smaller
# )

#Try cloud optimised Geotiff
terra::writeRaster(
  prob_rast.dens,
  filename  = "outputs/COTS_density.tif",
  overwrite = TRUE,
  filetype  = "COG",                       # use the COG driver
  gdal      = c(
    "COMPRESS=LZW",                        # lossless compression
    "BLOCKSIZE=512"                        # tile size (COG default is 512)
  )
)

```

